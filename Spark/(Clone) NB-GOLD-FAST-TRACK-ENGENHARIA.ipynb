{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11c2571-b2e3-488c-bbce-1727ec658196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**INICIO DO PROCESSO E CONFIGURA√á√ÉO DAS PASTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c87c5ff-fb18-4e83-ba1f-31ab5b40ddba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configura√ß√£o de diret√≥rios\n",
    "gold_path = \"/Volumes/workspace/default/gold\"\n",
    "delta_table_path = \"/Volumes/workspace/default/delta/final_climate_data\"\n",
    "delta_catalog = \"/Volumes/workspace/default\"\n",
    "delta_table_name = \"climate_data_final\"\n",
    "\n",
    "print(\"Configurando ambiente para cria√ß√£o da tabela Delta...\")\n",
    "print(f\"Gold path: {gold_path}\")\n",
    "print(f\"Delta path: {delta_table_path}\")\n",
    "print(f\"Catalog: {delta_catalog}\")\n",
    "print(f\"Table: {delta_table_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e82c99b-8c0e-4a50-900d-19dd9cdce7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**LENDO OS DADOS DA CAMADA GOLD E VISUALIZANDO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ca3178-5ea6-407c-9d3d-9f26b9029235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_gold_data():\n",
    "    \"\"\"L√™ todos os arquivos das camadas gold\"\"\"\n",
    "    try:\n",
    "        print(\"Lendo dados da camada Gold...\")\n",
    "        \n",
    "        # Listar arquivos gold\n",
    "        gold_files = dbutils.fs.ls(gold_path)\n",
    "        weather_files = [f.path for f in gold_files if 'weather_gold' in f.name]\n",
    "        air_quality_files = [f.path for f in gold_files if 'air_quality_gold' in f.name]\n",
    "        \n",
    "        print(f\" Arquivos de weather encontrados: {len(weather_files)}\")\n",
    "        print(f\" Arquivos de air quality encontrados: {len(air_quality_files)}\")\n",
    "        \n",
    "        # Ler dados meteorol√≥gicos\n",
    "        weather_dfs = []\n",
    "        for file in weather_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            weather_dfs.append(df)\n",
    "        \n",
    "        if weather_dfs:\n",
    "            weather_df = weather_dfs[0]\n",
    "            for df in weather_dfs[1:]:\n",
    "                weather_df = weather_df.union(df)\n",
    "            print(f\"Total de registros meteorol√≥gicos: {weather_df.count()}\")\n",
    "        else:\n",
    "            print(\" Nenhum dado meteorol√≥gico encontrado\")\n",
    "            weather_df = None\n",
    "        \n",
    "        # Ler dados de qualidade do ar\n",
    "        air_quality_dfs = []\n",
    "        for file in air_quality_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            air_quality_dfs.append(df)\n",
    "        \n",
    "        if air_quality_dfs:\n",
    "            air_quality_df = air_quality_dfs[0]\n",
    "            for df in air_quality_dfs[1:]:\n",
    "                air_quality_df = air_quality_df.union(df)\n",
    "            print(f\"üå´Ô∏è  Total de registros de qualidade do ar: {air_quality_df.count()}\")\n",
    "        else:\n",
    "            print(\" Nenhum dado de qualidade do ar encontrado\")\n",
    "            air_quality_df = None\n",
    "        \n",
    "        return weather_df, air_quality_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler dados gold: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Ler os dados\n",
    "weather_df, air_quality_df = read_gold_data()\n",
    "\n",
    "# Visualizar amostra dos dados\n",
    "if weather_df:\n",
    "    print(\" Amostra dos dados meteorol√≥gicos:\")\n",
    "    display(weather_df.limit(5))\n",
    "\n",
    "if air_quality_df:\n",
    "    print(\" Amostra dos dados de qualidade do ar:\")\n",
    "    display(air_quality_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f31d65-b1d7-4fda-b072-a980d586503b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**UNIFICANDO OS DADOS E SETANDO AS COLUNAS NECECSSARIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdea5148-2378-45d1-ba58-16beb55f0cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_final_table(weather_df, air_quality_df):\n",
    "    \"\"\"Cria a tabela final unificada\"\"\"\n",
    "    try:\n",
    "        print(\"Unificando dados...\")\n",
    "        \n",
    "        if weather_df is None or air_quality_df is None:\n",
    "            print(\"Dados insuficientes para unifica√ß√£o\")\n",
    "            return None\n",
    "        \n",
    "        # Juntar os dados por timestamp e localiza√ß√£o\n",
    "        final_df = weather_df.alias(\"w\").join(\n",
    "            air_quality_df.alias(\"a\"),\n",
    "            (col(\"w.timestamp\") == col(\"a.timestamp\")) & \n",
    "            (col(\"w.location\") == col(\"a.location\")),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"w.timestamp\"),\n",
    "            col(\"w.date\"),\n",
    "            col(\"w.temperature_2m\").alias(\"temperature_c\"),\n",
    "            col(\"w.relative_humidity_2m\").alias(\"humidity_percent\"),\n",
    "            col(\"w.pressure_msl\").alias(\"pressure_hpa\"),\n",
    "            col(\"w.precipitation\").alias(\"precipitation_mm\"),\n",
    "            col(\"a.pm10\"),\n",
    "            col(\"a.pm2_5\"),\n",
    "            col(\"a.carbon_monoxide\").alias(\"co_ugm3\"),\n",
    "            col(\"a.nitrogen_dioxide\").alias(\"no2_ugm3\"),\n",
    "            col(\"w.location\"),\n",
    "            col(\"w.latitude\"),\n",
    "            col(\"w.longitude\")\n",
    "        )\n",
    "        \n",
    "        # Adicionar colunas calculadas\n",
    "        final_df = final_df.withColumn(\n",
    "            \"temperature_f\", \n",
    "            (col(\"temperature_c\") * 9/5) + 32\n",
    "        ).withColumn(\n",
    "            \"air_quality_index\",\n",
    "            when(col(\"pm2_5\") <= 12, \"Bom\")\n",
    "            .when(col(\"pm2_5\") <= 35, \"Moderado\")\n",
    "            .when(col(\"pm2_5\") <= 55, \"Ruim\")\n",
    "            .when(col(\"pm2_5\") <= 150, \"Muito Ruim\")\n",
    "            .otherwise(\"Perigoso\")\n",
    "        ).withColumn(\n",
    "            \"load_timestamp\", \n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        print(f\"Dados unificados com sucesso!\")\n",
    "        print(f\"Total de registros na tabela final: {final_df.count()}\")\n",
    "        print(f\"Per√≠odo dos dados: {final_df.agg(min('timestamp'), max('timestamp')).collect()[0]}\")\n",
    "        \n",
    "        # Mostrar schema final\n",
    "        print(\"  Schema da tabela final:\")\n",
    "        final_df.printSchema()\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na unifica√ß√£o dos dados: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Criar tabela final\n",
    "final_table_df = create_final_table(weather_df, air_quality_df)\n",
    "\n",
    "if final_table_df:\n",
    "    print(\" Amostra da tabela final:\")\n",
    "    display(final_table_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "634b3a81-7d9a-43cd-98f6-091a3a1c0170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**V2 AJUSTADA LENDO AS BIBLIOTECAS E DIRETORIOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2fedaf-c427-4572-b519-365945eb92cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configura√ß√£o de diret√≥rios\n",
    "gold_path = \"/Volumes/workspace/default/gold\"\n",
    "delta_table_path = \"/Volumes/workspace/default/delta/final_climate_data\"\n",
    "delta_catalog = \"/Volumes/workspace/default\"\n",
    "delta_table_name = \"climate_data_final\"\n",
    "\n",
    "print(\"Configurando ambiente para cria√ß√£o da tabela Delta...\")\n",
    "print(f\"Gold path: {gold_path}\")\n",
    "print(f\"Delta path: {delta_table_path}\")\n",
    "print(f\"Catalog: {delta_catalog}\")\n",
    "print(f\"Table: {delta_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9178e29-b2cc-43ec-9ff6-7e68e42bf3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**LENDO OS DADOS  DA CAMADA GOLD E VISUALIZANDO OS DADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e233ca4-2f26-4af9-b57e-4a588923a0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_gold_data():\n",
    "    \"\"\"L√™ todos os arquivos das camadas gold\"\"\"\n",
    "    try:\n",
    "        print(\" Lendo dados da camada Gold...\")\n",
    "        \n",
    "        # Listar arquivos gold usando a sintaxe correta\n",
    "        gold_files = dbutils.fs.ls(gold_path.replace(\"dbfs:\", \"\"))\n",
    "        weather_files = [f.path for f in gold_files if 'weather_gold' in f.name]\n",
    "        air_quality_files = [f.path for f in gold_files if 'air_quality_gold' in f.name]\n",
    "        \n",
    "        print(f\"  Arquivos de weather encontrados: {len(weather_files)}\")\n",
    "        print(f\" Arquivos de air quality encontrados: {len(air_quality_files)}\")\n",
    "        \n",
    "        # Ler dados meteorol√≥gicos\n",
    "        weather_dfs = []\n",
    "        for file in weather_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            weather_dfs.append(df)\n",
    "        \n",
    "        if weather_dfs:\n",
    "            weather_df = weather_dfs[0]\n",
    "            for df in weather_dfs[1:]:\n",
    "                weather_df = weather_df.union(df)\n",
    "            print(f\"  Total de registros meteorol√≥gicos: {weather_df.count()}\")\n",
    "        else:\n",
    "            print(\"  Nenhum dado meteorol√≥gico encontrado\")\n",
    "            weather_df = None\n",
    "        \n",
    "        # Ler dados de qualidade do ar\n",
    "        air_quality_dfs = []\n",
    "        for file in air_quality_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            air_quality_dfs.append(df)\n",
    "        \n",
    "        if air_quality_dfs:\n",
    "            air_quality_df = air_quality_dfs[0]\n",
    "            for df in air_quality_dfs[1:]:\n",
    "                air_quality_df = air_quality_df.union(df)\n",
    "            print(f\"üå´Ô∏è  Total de registros de qualidade do ar: {air_quality_df.count()}\")\n",
    "        else:\n",
    "            print(\"  Nenhum dado de qualidade do ar encontrado\")\n",
    "            air_quality_df = None\n",
    "        \n",
    "        return weather_df, air_quality_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao ler dados gold: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Ler os dados\n",
    "weather_df, air_quality_df = read_gold_data()\n",
    "\n",
    "# Visualizar amostra dos dados\n",
    "if weather_df:\n",
    "    print(\" Amostra dos dados meteorol√≥gicos:\")\n",
    "    display(weather_df.limit(5))\n",
    "\n",
    "if air_quality_df:\n",
    "    print(\" Amostra dos dados de qualidade do ar:\")\n",
    "    display(air_quality_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99f980b-8cbb-4a41-a79f-e4229b1619fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "****CRIA√á√ÉO DA TABELA  APLICANDO O JOIN** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d215e4df-9428-49ec-982c-d2f15fdd2ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_final_table(weather_df, air_quality_df):\n",
    "    \"\"\"Cria a tabela final unificada\"\"\"\n",
    "    try:\n",
    "        print(\"Unificando dados...\")\n",
    "        \n",
    "        if weather_df is None or air_quality_df is None:\n",
    "            print(\" Dados insuficientes para unifica√ß√£o\")\n",
    "            return None\n",
    "        \n",
    "        # Juntar os dados por timestamp e localiza√ß√£o \n",
    "        final_df = weather_df.alias(\"w\").join(\n",
    "            air_quality_df.alias(\"a\"),\n",
    "            (col(\"w.timestamp\") == col(\"a.timestamp\")) & \n",
    "            (col(\"w.location\") == col(\"a.location\")),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"w.timestamp\"),\n",
    "            col(\"w.date\"),\n",
    "            col(\"w.temperature_2m\").alias(\"temperature_c\"),\n",
    "            col(\"w.relative_humidity_2m\").alias(\"humidity_percent\"),\n",
    "            col(\"w.pressure_msl\").alias(\"pressure_hpa\"),\n",
    "            col(\"w.precipitation\").alias(\"precipitation_mm\"),\n",
    "            col(\"a.pm10\"),\n",
    "            col(\"a.pm2_5\"),\n",
    "            col(\"a.carbon_monoxide\").alias(\"co_ugm3\"),\n",
    "            col(\"a.nitrogen_dioxide\").alias(\"no2_ugm3\"),\n",
    "            col(\"w.location\"),\n",
    "            col(\"w.latitude\"),\n",
    "            col(\"w.longitude\")\n",
    "           \n",
    "        )\n",
    "        \n",
    "        # Adicionar colunas calculadas \n",
    "        final_df = final_df.withColumn(  \n",
    "            \"temperature_f\", \n",
    "            (col(\"temperature_c\") * 9/5) + 32\n",
    "        ).withColumn(\n",
    "            \"air_quality_index\",\n",
    "            when(col(\"pm2_5\") <= 12, \"Bom\")\n",
    "            .when(col(\"pm2_5\") <= 35, \"Moderado\")\n",
    "            .when(col(\"pm2_5\") <= 55, \"Ruim\")\n",
    "            .when(col(\"pm2_5\") <= 150, \"Muito Ruim\")\n",
    "            .otherwise(\"Perigoso\")\n",
    "        ).withColumn(\n",
    "            \"load_timestamp\", \n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        print(f\" Dados unificados com sucesso!\")\n",
    "        print(f\"Total de registros na tabela final: {final_df.count()}\")\n",
    "        \n",
    "        # Mostrar schema final\n",
    "        print(\" Schema da tabela final:\")\n",
    "        final_df.printSchema()\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro na unifica√ß√£o dos dados: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Criar tabela final\n",
    "final_table_df = create_final_table(weather_df, air_quality_df)\n",
    "\n",
    "if final_table_df:\n",
    "    print(\"Amostra da tabela final:\")\n",
    "    display(final_table_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b94ebe5-89c1-4450-ba34-7252918527be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CRIANDO  TABELA FINAL USANDO SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fcc004f-ee1c-4bfb-bb8f-f61260317071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_delta_table(final_df):\n",
    "    \"\"\"Cria a tabela Delta final \"\"\"\n",
    "    try:\n",
    "        print(\" Criando tabela Delta...\")\n",
    "        \n",
    "        if final_df is None:\n",
    "            print(\" Nenhum dado para criar tabela\")\n",
    "            return None\n",
    "        \n",
    "        #  Unity Catalog\n",
    "        \n",
    "        catalog_name = \"workspace\"  #  catalog\n",
    "        schema_name = \"default\"     #  schema\n",
    "        table_name = \"climate_data_final\"\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Criar DataFrame tempor√°rio\n",
    "        final_df.createOrReplaceTempView(\"temp_final_table\")\n",
    "        \n",
    "        # Criar tabela usando SQL \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {full_table_name}\n",
    "            USING DELTA\n",
    "            AS SELECT * FROM temp_final_table\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\" Tabela Delta criada com sucesso!\")\n",
    "        print(f\"Tabela: {full_table_name}\")\n",
    "        \n",
    "        # Verificar a tabela criada\n",
    "        delta_df = spark.read.table(full_table_name)\n",
    "        print(f\"Total de registros na tabela Delta: {delta_df.count()}\")\n",
    "        \n",
    "        return delta_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar tabela Delta: {str(e)}\")\n",
    "        print(\" Tentando m√©todo alternativo...\")\n",
    "        \n",
    "        # M√©todo alternativo usando DataFrameWriter\n",
    "        try:\n",
    "            # Escrever usando DataFrameWriter\n",
    "            (final_df.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"overwrite\")\n",
    "                .option(\"overwriteSchema\", \"true\")\n",
    "                .saveAsTable(full_table_name))\n",
    "            \n",
    "            print(f\" Tabela Delta criada com m√©todo alternativo!\")\n",
    "            delta_df = spark.read.table(full_table_name)\n",
    "            return delta_df\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\" Erro no m√©todo alternativo: {str(e2)}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f69a0d19-4b6e-4c25-93d4-d342b6759e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CRIANDO A TABELA DELTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e4d8a4-3c33-41f9-b745-1a661654d3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configura√ß√£o \n",
    "catalog_name = \"workspace\"  # Seu catalog\n",
    "schema_name = \"default\"     # Seu schema\n",
    "table_name = \"climate_data_final\"\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "print(\"Configura√ß√£o para Unity Catalog:\")\n",
    "print(f\"Catalog: {catalog_name}\")\n",
    "print(f\" Schema: {schema_name}\")\n",
    "print(f\"Table: {table_name}\")\n",
    "print(f\" Full name: {full_table_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4.2 Executar Cria√ß√£o da Tabela\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Criar tabela Delta\n",
    "delta_final_df = create_delta_table(final_table_df)\n",
    "\n",
    "if delta_final_df:\n",
    "    print(\" Amostra da tabela Delta:\")\n",
    "    display(delta_final_df.limit(10))\n",
    "else:\n",
    "    print(\"Falha ao criar tabela Delta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7254a7-4cd1-4150-bbd4-9c0d4014bd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b0a4ed-cea4-4ed7-baeb-bf06b2711336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CHECKLIST PARA VERIFICA√á√ÉO DA CRIA√á√ÉO DA TABELA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145c3e7-5a08-4348-b264-3d606bb9e44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar se a tabela foi criada corretamente\n",
    "try:\n",
    "    # Listar tabelas no schema\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name} LIKE '{table_name}'\")\n",
    "    if tables.count() > 0:\n",
    "        print(\"Tabela encontrada no Unity Catalog!\")\n",
    "        display(tables)\n",
    "        \n",
    "        # Detalhes da tabela\n",
    "        table_details = spark.sql(f\"DESCRIBE DETAIL {full_table_name}\")\n",
    "        print(\"Detalhes da tabela:\")\n",
    "        display(table_details)\n",
    "        \n",
    "        # Contagem de registros\n",
    "        count = spark.sql(f\"SELECT COUNT(*) FROM {full_table_name}\").collect()[0][0]\n",
    "        print(f\" Total de registros: {count:,}\")\n",
    "        \n",
    "    else:\n",
    "        print(\" Tabela n√£o encontrada no Unity Catalog\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Erro ao verificar tabela: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e84ab1-c259-41da-87ee-eaa16ce3f968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**QUERYS SQL PARA CONSULTA DOS DADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ee7c73-8c64-413b-a787-3deed57ae9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# consulta na tabela\n",
    "try:\n",
    "    print(\"Testando consulta na tabela...\")\n",
    "    test_query = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            temperature_c,\n",
    "            humidity_percent,\n",
    "            pm2_5,\n",
    "            air_quality_index\n",
    "        FROM {full_table_name}\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    display(test_query)\n",
    "    print(\"Consulta executada com sucesso!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro na consulta: {str(e)}\")\n",
    "    print(\" Tentando ver tabelas dispon√≠veis...\")\n",
    "    \n",
    "    # Listar todas as tabelas dispon√≠veis\n",
    "    try:\n",
    "        all_tables = spark.sql(\"SHOW TABLES\")\n",
    "        display(all_tables)\n",
    "    except:\n",
    "        print(\"N√£o foi poss√≠vel listar tabelas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c740a7b-af7b-46cf-bf92-e81218fc114d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL : Dados mais recentes\n",
    "display(\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            timestamp,\n",
    "            temperature_c,\n",
    "            humidity_percent,\n",
    "            pm2_5,\n",
    "            air_quality_index\n",
    "        FROM climate_data_final\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd05be89-da36-4d4b-ac53-945899fff08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: M√©dias por hora\n",
    "print(\"M√©dias por hora:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        DATE_TRUNC('hour', timestamp) as hour,\n",
    "        ROUND(AVG(temperature_c), 1) as avg_temperature,\n",
    "        ROUND(AVG(pm2_5), 1) as avg_pm2_5,\n",
    "        COUNT(*) as readings\n",
    "    FROM climate_data_final\n",
    "    GROUP BY DATE_TRUNC('hour', timestamp)\n",
    "    ORDER BY hour DESC\n",
    "    LIMIT 12\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66f41d4-26ae-4914-8ad9-db22927462d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SQL: Qualidade do ar por faixa\n",
    "print(\"Qualidade do ar por faixa:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        air_quality_index,\n",
    "        MIN(pm2_5) as min_pm2_5,\n",
    "        MAX(pm2_5) as max_pm2_5,\n",
    "        AVG(pm2_5) as avg_pm2_5,\n",
    "        COUNT(*) as readings\n",
    "    FROM climate_data_final\n",
    "    GROUP BY air_quality_index\n",
    "    ORDER BY avg_pm2_5\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) NB-GOLD-FAST-TRACK-ENGENHARIA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
