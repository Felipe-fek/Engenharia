{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63dd6e04-ba52-4d28-9c1f-e541c862e8df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    " \n",
    "def get_pokemon_details(poke_id):\n",
    "    url = f\"https://pokeapi.co/api/v2/pokemon/{poke_id}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    " \n",
    "    data = response.json()\n",
    "    stats_dict = {s[\"stat\"][\"name\"]: s[\"base_stat\"] for s in data.get(\"stats\", [])}\n",
    " \n",
    "    return {\n",
    "        \"id\": data.get(\"id\"),\n",
    "        \"name\": data.get(\"name\"),\n",
    "        \"base_experience\": data.get(\"base_experience\"),\n",
    "        \"height\": data.get(\"height\"),\n",
    "        \"is_default\": data.get(\"is_default\"),\n",
    "        \"order\": data.get(\"order\"),\n",
    "        \"weight\": data.get(\"weight\"),\n",
    "        \"abilities\": [ab[\"ability\"][\"name\"] for ab in data.get(\"abilities\", [])],\n",
    "        \"forms\": [f[\"name\"] for f in data.get(\"forms\", [])],\n",
    "        \"game_indices\": [gi[\"version\"][\"name\"] for gi in data.get(\"game_indices\", [])],\n",
    "        \"held_items\": [hi[\"item\"][\"name\"] for hi in data.get(\"held_items\", [])],\n",
    "        \"location_area_encounters\": data.get(\"location_area_encounters\"),\n",
    "        \"moves\": [m[\"move\"][\"name\"] for m in data.get(\"moves\", [])],\n",
    "        \"sprites\": data.get(\"sprites\", {}).get(\"front_default\"),\n",
    "        \"species\": data.get(\"species\", {}).get(\"name\"),\n",
    "        \"types\": [t[\"type\"][\"name\"] for t in data.get(\"types\", [])],\n",
    "        **stats_dict\n",
    "    }\n",
    " \n",
    "# Descobre o número total de Pokémons disponíveis\n",
    "def get_total_pokemon():\n",
    "    url = \"https://pokeapi.co/api/v2/pokemon?limit=1\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()[\"count\"]\n",
    " \n",
    "# Pega todos os Pokémons\n",
    "total = get_total_pokemon()\n",
    "pokemon_list = []\n",
    " \n",
    "for i in range(1, total + 1):\n",
    "    print(f\"Buscando Pokémon {i}/{total}\")\n",
    "    pokemon = get_pokemon_details(i)\n",
    "    if pokemon:\n",
    "        pokemon_list.append(pokemon)\n",
    "    time.sleep(0.2)  # Respeita rate limit da API\n",
    " \n",
    "# Cria DataFrame e exporta para CSV\n",
    "df = pd.DataFrame(pokemon_list)\n",
    "df.dispay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b7171a3-b555-4346-bec2-7bee0a0d6ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CONFIGURAÇÃO DE DIRETORIOS ESTRUTURA MEDELHÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2cde992-e900-440b-932d-99425b0fb968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configuração de diretórios \n",
    "\n",
    "bronze_path = \"/Volumes/workspace/default/bronze/\"\n",
    "silver_path = \"/Volumes/workspace/default/silver/\"\n",
    "gold_path = \"/Volumes/workspace/default/gold/\"\n",
    "\n",
    "# Criar diretórios se não existirem\n",
    "os.makedirs(bronze_path, exist_ok=True)\n",
    "os.makedirs(silver_path, exist_ok=True)\n",
    "os.makedirs(gold_path, exist_ok=True)\n",
    "\n",
    "# Coordenadas de São Paulo\n",
    "LATITUDE = -23.55\n",
    "LONGITUDE = -46.63\n",
    "\n",
    "print(\"Diretórios configurados:\")\n",
    "print(f\"Bronze: {bronze_path}\")\n",
    "print(f\"Silver: {silver_path}\")\n",
    "print(f\"Gold: {gold_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45662216-f4c1-41c5-b7bd-fe92414f12db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**EXTRAIR DADOS DA API DO TEMPO E SALVAR NA CAMADA BRONZE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009b8c4b-5771-4786-8981-c469c7a9aac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_weather_data():\n",
    "    \"\"\"Extrai dados meteorológicos da Open-Meteo API\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            'latitude': LATITUDE,\n",
    "            'longitude': LONGITUDE,\n",
    "            'hourly': 'temperature_2m,relative_humidity_2m,pressure_msl,precipitation',\n",
    "            'timezone': 'America/Sao_Paulo',\n",
    "            'forecast_days': 1\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Salvar dados brutos na camada bronze\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{bronze_path}/weather_data_{timestamp}.json\"\n",
    "        \n",
    "        # Usar dbutils para salvar no Volume\n",
    "        dbutils.fs.put(filename, json.dumps(data), overwrite=True)\n",
    "            \n",
    "        print(f\"Dados meteorológicos salvos em: {filename}\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na extração de dados meteorológicos: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_air_quality_data():\n",
    "    \"\"\"Extrai dados de qualidade do ar da OpenAQ API\"\"\"\n",
    "    try:\n",
    "        url = \"https://api.openaq.org/v2/latest\"\n",
    "        params = {\n",
    "            'coordinates': f\"{LATITUDE},{LONGITUDE}\",\n",
    "            'radius': 10000,\n",
    "            'limit': 100\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'accept': 'application/json'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Salvar dados brutos na camada bronze\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{bronze_path}/air_quality_data_{timestamp}.json\"\n",
    "        \n",
    "        # Usar dbutils para salvar no Volume\n",
    "        dbutils.fs.put(filename, json.dumps(data), overwrite=True)\n",
    "            \n",
    "        print(f\"Dados de qualidade do ar salvos em: {filename}\")\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na extração de dados de qualidade do ar: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b59a4d6-77b3-4a8f-8a5e-34061fb03167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**TRANSFORMAÇÕES TRATAMENTO DOS DADOS E REMOÇÃO DE NULOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d68981a6-7b18-406d-8a07-a0330db8b9c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_weather_data(weather_data):\n",
    "    \"\"\"Transforma dados meteorológicos usando Pandas\"\"\"\n",
    "    if not weather_data:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extrair dados horários\n",
    "        hourly_data = weather_data.get('hourly', {})\n",
    "        \n",
    "        # Criar DataFrame Pandas\n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': hourly_data.get('time', []),\n",
    "            'temperature_2m': hourly_data.get('temperature_2m', []),\n",
    "            'relative_humidity_2m': hourly_data.get('relative_humidity_2m', []),\n",
    "            'pressure_msl': hourly_data.get('pressure_msl', []),\n",
    "            'precipitation': hourly_data.get('precipitation', [])\n",
    "        })\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['temperature_2m'] = pd.to_numeric(df['temperature_2m'], errors='coerce')\n",
    "        df['relative_humidity_2m'] = pd.to_numeric(df['relative_humidity_2m'], errors='coerce')\n",
    "        df['pressure_msl'] = pd.to_numeric(df['pressure_msl'], errors='coerce')\n",
    "        df['precipitation'] = pd.to_numeric(df['precipitation'], errors='coerce')\n",
    "        df['extraction_timestamp'] = datetime.now()\n",
    "        df['latitude'] = LATITUDE\n",
    "        df['longitude'] = LONGITUDE\n",
    "        df['location'] = \"São Paulo\"\n",
    "        \n",
    "        # Remover valores nulos\n",
    "        df = df.dropna(subset=['temperature_2m', 'timestamp'])\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na transformação de dados meteorológicos: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def transform_air_quality_data(air_quality_data):\n",
    "    \"\"\"Transforma dados de qualidade do ar usando Pandas\"\"\"\n",
    "    if not air_quality_data:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Extrair resultados\n",
    "        results = air_quality_data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            return None\n",
    "        \n",
    "        # Processar cada localização\n",
    "        processed_data = []\n",
    "        for result in results:\n",
    "            location = result.get('location', '')\n",
    "            measurements = result.get('measurements', [])\n",
    "            \n",
    "            for measurement in measurements:\n",
    "                processed_data.append({\n",
    "                    'location': location,\n",
    "                    'parameter': measurement.get('parameter', ''),\n",
    "                    'value': measurement.get('value', 0),\n",
    "                    'unit': measurement.get('unit', ''),\n",
    "                    'last_updated': measurement.get('lastUpdated', ''),\n",
    "                    'latitude': LATITUDE,\n",
    "                    'longitude': LONGITUDE\n",
    "                })\n",
    "        \n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        df['last_updated'] = pd.to_datetime(df['last_updated'])\n",
    "        df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        df['extraction_timestamp'] = datetime.now()\n",
    "        df['date'] = datetime.now().date()\n",
    "        df['parameter'] = df['parameter'].str.lower()\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na transformação de dados de qualidade do ar: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870a58a5-e32b-4869-86bd-264eb8401ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**QUALIDADE DOS DADOS - METRICAS DE QUALIDADE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c30cc95f-1d0d-41a9-8020-66f3b4183189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_quality_report(df, source_name):\n",
    "    \"\"\"Gera relatório de qualidade de dados\"\"\"\n",
    "    try:\n",
    "        # Calcular métricas de qualidade\n",
    "        total_records = len(df)\n",
    "        null_counts = {}\n",
    "        \n",
    "        for column in df.columns:\n",
    "            null_counts[column] = df[column].isnull().sum()\n",
    "        \n",
    "        # Criar relatório\n",
    "        report = {\n",
    "            \"source\": source_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_records\": total_records,\n",
    "            \"null_counts\": null_counts,\n",
    "            \"columns\": list(df.columns),\n",
    "            \"data_types\": {col: str(dtype) for col, dtype in df.dtypes.items()}\n",
    "        }\n",
    "        \n",
    "        # Salvar relatório\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_filename = f\"{bronze_path}/quality_report_{source_name}_{timestamp}.json\"\n",
    "        \n",
    "        # Usar dbutils para salvar no Volume\n",
    "        dbutils.fs.put(report_filename, json.dumps(report, indent=2), overwrite=True)\n",
    "            \n",
    "        print(f\"Relatório de qualidade salvo em: {report_filename}\")\n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na geração do relatório de qualidade: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1e596e-5de1-45e3-af15-58472d3ce6d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**PIPELINE PRINCIPAL ( ETL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b040a2-97bf-4dbc-a051-7ff710d94dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Executa o pipeline completo\"\"\"\n",
    "    print(\"Iniciando pipeline de dados...\")\n",
    "    \n",
    "    # Extração\n",
    "    print(\"Extraindo dados meteorológicos...\")\n",
    "    weather_data = extract_weather_data()\n",
    "    \n",
    "    print(\"Extraindo dados de qualidade do ar...\")\n",
    "    air_quality_data = extract_air_quality_data()\n",
    "    \n",
    "    # Transformação\n",
    "    print(\"Transformando dados meteorológicos...\")\n",
    "    weather_df = transform_weather_data(weather_data)\n",
    "    \n",
    "    print(\"Transformando dados de qualidade do ar...\")\n",
    "    air_quality_df = transform_air_quality_data(air_quality_data)\n",
    "    \n",
    "    # Qualidade de dados\n",
    "    if weather_df is not None:\n",
    "        generate_data_quality_report(weather_df, \"weather_data\")\n",
    "        \n",
    "        # Salvar na camada silver (Parquet)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        silver_weather_file = f\"{silver_path}/weather_data_{timestamp}.parquet\"\n",
    "        \n",
    "        # Salvar usando Spark para garantir compatibilidade com Volumes\n",
    "        spark_df = spark.createDataFrame(weather_df)\n",
    "        spark_df.write.mode(\"overwrite\").format(\"parquet\").save(silver_weather_file)\n",
    "        print(f\"Dados meteorológicos salvos na silver: {silver_weather_file}\")\n",
    "    \n",
    "    if air_quality_df is not None:\n",
    "        generate_data_quality_report(air_quality_df, \"air_quality_data\")\n",
    "        \n",
    "        # Salvar na camada silver (Parquet)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        silver_air_quality_file = f\"{silver_path}/air_quality_data_{timestamp}.parquet\"\n",
    "        \n",
    "        # Salvar usando Spark\n",
    "        spark_df = spark.createDataFrame(air_quality_df)\n",
    "        spark_df.write.mode(\"overwrite\").format(\"parquet\").save(silver_air_quality_file)\n",
    "        print(f\"Dados de qualidade do ar salvos na silver: {silver_air_quality_file}\")\n",
    "    \n",
    "    # Criar dados gold (se ambos os dados estiverem disponíveis)\n",
    "    if weather_df is not None and air_quality_df is not None:\n",
    "        create_gold_data(weather_df, air_quality_df)\n",
    "    \n",
    "    print(\"Pipeline concluído com sucesso!\")\n",
    "\n",
    "def create_gold_data(weather_df, air_quality_df):\n",
    "    \"\"\"Cria dados finais na camada gold\"\"\"\n",
    "    try:\n",
    "        # Dados meteorológicos gold\n",
    "        weather_gold = weather_df[['timestamp', 'date', 'temperature_2m', \n",
    "                                 'relative_humidity_2m', 'pressure_msl', \n",
    "                                 'precipitation', 'location', 'latitude', 'longitude']].copy()\n",
    "        \n",
    "        # Dados qualidade do ar gold (agregação)\n",
    "        air_quality_gold = air_quality_df.groupby(['parameter', 'date']).agg({\n",
    "            'last_updated': 'max',\n",
    "            'value': 'mean',\n",
    "            'unit': 'first'\n",
    "        }).reset_index()\n",
    "        air_quality_gold.rename(columns={'parameter': 'pollutant'}, inplace=True)\n",
    "        \n",
    "        # Salvar dados gold\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        gold_weather_file = f\"{gold_path}/weather_gold_{timestamp}.parquet\"\n",
    "        gold_air_quality_file = f\"{gold_path}/air_quality_gold_{timestamp}.parquet\"\n",
    "        \n",
    "        # Salvar usando Spark\n",
    "        spark_weather = spark.createDataFrame(weather_gold)\n",
    "        spark_air_quality = spark.createDataFrame(air_quality_gold)\n",
    "        \n",
    "        spark_weather.write.mode(\"overwrite\").format(\"parquet\").save(gold_weather_file)\n",
    "        spark_air_quality.write.mode(\"overwrite\").format(\"parquet\").save(gold_air_quality_file)\n",
    "        \n",
    "        print(\"Dados gold criados com sucesso!\")\n",
    "        print(f\"Weather Gold: {gold_weather_file}\")\n",
    "        print(f\"Air Quality Gold: {gold_air_quality_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na criação dos dados gold: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8a6d4ef-ba9f-4759-bd14-57d4f5012344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**EXECUTAR PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7fac2f8-ed84-47d3-b987-4304ae64e6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Executar pipeline\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56d085cd-1e63-4f75-9ec5-040bb2222918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CHECKLIST PARA VERIFICAR SE OS ARQUIVOS TRATADOS FORAM SALVOS CORRETAMENTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0604f3d3-1543-4118-8b75-5462e70f603d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_files(directory):\n",
    "    \"\"\"Lista arquivos no diretório\"\"\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(directory)\n",
    "        print(f\"Arquivos em {directory}:\")\n",
    "        for file in files:\n",
    "            print(f\"  - {file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao listar {directory}: {str(e)}\")\n",
    "\n",
    "print(\"Verificando arquivos criados...\")\n",
    "list_files(bronze_path)\n",
    "list_files(silver_path)\n",
    "list_files(gold_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e331371-77fc-4722-865c-8f75183defd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    gold_files = dbutils.fs.ls(gold_path)\n",
    "    if gold_files:\n",
    "        sample_file = gold_files[0].path\n",
    "        print(f\"Amostra do arquivo: {sample_file}\")\n",
    "        \n",
    "        if sample_file.endswith('.parquet'):\n",
    "            df_sample = spark.read.parquet(sample_file)\n",
    "            display(df_sample.limit(5))\n",
    "    else:\n",
    "        print(\"Execute o pipeline primeiro para gerar os dados gold\")\n",
    "except Exception as e:\n",
    "    print(f\" {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63477019-03e1-49c8-b5fc-295e57e612ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_weather_data():\n",
    "    \"\"\"Extrai dados meteorológicos da Open-Meteo API\"\"\"\n",
    "    try:\n",
    "        print(\"Extraindo dados meteorológicos\")\n",
    "        url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "        params = {\n",
    "            'latitude': LATITUDE,\n",
    "            'longitude': LONGITUDE,\n",
    "            'hourly': 'temperature_2m,relative_humidity_2m,pressure_msl,precipitation',\n",
    "            'timezone': 'America/Sao_Paulo',\n",
    "            'forecast_days': 1\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Salvar dados brutos na camada bronze\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{bronze_path}/weather_data_{timestamp}.json\"\n",
    "        \n",
    "        # Usar dbutils para salvar no Volume\n",
    "        dbutils.fs.put(filename, json.dumps(data), overwrite=True)\n",
    "            \n",
    "        print(f\"Dados meteorológicos salvos em: {filename}\")\n",
    "        \n",
    "        # Visualizar estrutura dos dados brutos\n",
    "        print(\"Estrutura dos dados meteorológicos brutos:\")\n",
    "        print(f\"Chaves disponíveis: {list(data.keys())}\")\n",
    "        if 'hourly' in data:\n",
    "            print(f\"Colunas hourly: {list(data['hourly'].keys())}\")\n",
    "            print(f\"Número de registros: {len(data['hourly'].get('time', []))}\")\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na extração de dados meteorológicos: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_air_quality_data():\n",
    "    \"\"\"Extrai dados de qualidade do ar usando API alternativa\"\"\"\n",
    "    try:\n",
    "        print(\"Extraindo dados de qualidade do ar...\")\n",
    "        \n",
    "        # Usando API de qualidade do ar do Open-Meteo\n",
    "        url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "        params = {\n",
    "            'latitude': LATITUDE,\n",
    "            'longitude': LONGITUDE,\n",
    "            'hourly': 'pm10,pm2_5,carbon_monoxide,nitrogen_dioxide',\n",
    "            'timezone': 'America/Sao_Paulo',\n",
    "            'forecast_days': 1\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Salvar dados brutos na camada bronze\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{bronze_path}/air_quality_data_{timestamp}.json\"\n",
    "        \n",
    "        # Usar dbutils para salvar no Volume\n",
    "        dbutils.fs.put(filename, json.dumps(data), overwrite=True)\n",
    "            \n",
    "        print(f\"ados de qualidade do ar salvos em: {filename}\")\n",
    "        \n",
    "        # Visualizar estrutura dos dados brutos\n",
    "        print(\"Estrutura dos dados de qualidade do ar brutos:\")\n",
    "        print(f\"Chaves disponíveis: {list(data.keys())}\")\n",
    "        if 'hourly' in data:\n",
    "            print(f\"Colunas hourly: {list(data['hourly'].keys())}\")\n",
    "            print(f\"Número de registros: {len(data['hourly'].get('time', []))}\")\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na extração de dados de qualidade do ar: {str(e)}\")\n",
    "        print(\"Criando dados de exemplo para demonstração...\")\n",
    "        \n",
    "        # Criar dados de exemplo para continuar o pipeline\n",
    "        example_data = {\n",
    "            \"hourly\": {\n",
    "                \"time\": [(datetime.now() - pd.Timedelta(hours=i)).isoformat() for i in range(24)],\n",
    "                \"pm10\": [25.5 + i for i in range(24)],\n",
    "                \"pm2_5\": [15.2 + i*0.5 for i in range(24)],\n",
    "                \"carbon_monoxide\": [0.8 + i*0.1 for i in range(24)],\n",
    "                \"nitrogen_dioxide\": [12.3 + i*0.8 for i in range(24)]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"{bronze_path}/air_quality_data_example_{timestamp}.json\"\n",
    "        dbutils.fs.put(filename, json.dumps(example_data), overwrite=True)\n",
    "        \n",
    "        print(f\"Dados de exemplo salvos em: {filename}\")\n",
    "        return example_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17c7947f-a389-45c0-9f5c-1b2884c2281c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**TRATAMENTO V2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0a2608-698f-477f-8b6f-24689a02d24d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_weather_data(weather_data):\n",
    "    \"\"\"Transforma dados meteorológicos usando Pandas\"\"\"\n",
    "    if not weather_data:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"Transformando dados meteorológicos...\")\n",
    "        # Extrair dados horários\n",
    "        hourly_data = weather_data.get('hourly', {})\n",
    "        \n",
    "        # Criar DataFrame Pandas\n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': hourly_data.get('time', []),\n",
    "            'temperature_2m': hourly_data.get('temperature_2m', []),\n",
    "            'relative_humidity_2m': hourly_data.get('relative_humidity_2m', []),\n",
    "            'pressure_msl': hourly_data.get('pressure_msl', []),\n",
    "            'precipitation': hourly_data.get('precipitation', [])\n",
    "        })\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['temperature_2m'] = pd.to_numeric(df['temperature_2m'], errors='coerce')\n",
    "        df['relative_humidity_2m'] = pd.to_numeric(df['relative_humidity_2m'], errors='coerce')\n",
    "        df['pressure_msl'] = pd.to_numeric(df['pressure_msl'], errors='coerce')\n",
    "        df['precipitation'] = pd.to_numeric(df['precipitation'], errors='coerce')\n",
    "        df['extraction_timestamp'] = datetime.now()\n",
    "        df['latitude'] = LATITUDE\n",
    "        df['longitude'] = LONGITUDE\n",
    "        df['location'] = \"São Paulo\"\n",
    "        \n",
    "        # Remover valores nulos\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['temperature_2m', 'timestamp'])\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Dados meteorológicos transformados:\")\n",
    "        print(f\"Registros iniciais: {initial_count}\")\n",
    "        print(f\"Registros após limpeza: {final_count}\")\n",
    "        print(f\"Colunas: {list(df.columns)}\")\n",
    "        print(f\"Período: {df['timestamp'].min()} até {df['timestamp'].max()}\")\n",
    "        \n",
    "        # Visualizar amostra dos dados transformados\n",
    "        print(\" Amostra dos dados meteorológicos transformados:\")\n",
    "        display(df.head(5))\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na transformação de dados meteorológicos: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def transform_air_quality_data(air_quality_data):\n",
    "    \"\"\"Transforma dados de qualidade do ar usando Pandas\"\"\"\n",
    "    if not air_quality_data:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"Transformando dados de qualidade do ar...\")\n",
    "        # Extrair dados horários\n",
    "        hourly_data = air_quality_data.get('hourly', {})\n",
    "        \n",
    "        # Criar DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': hourly_data.get('time', []),\n",
    "            'pm10': hourly_data.get('pm10', []),\n",
    "            'pm2_5': hourly_data.get('pm2_5', []),\n",
    "            'carbon_monoxide': hourly_data.get('carbon_monoxide', []),\n",
    "            'nitrogen_dioxide': hourly_data.get('nitrogen_dioxide', [])\n",
    "        })\n",
    "        \n",
    "        # Aplicar transformações\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        df['pm10'] = pd.to_numeric(df['pm10'], errors='coerce')\n",
    "        df['pm2_5'] = pd.to_numeric(df['pm2_5'], errors='coerce')\n",
    "        df['carbon_monoxide'] = pd.to_numeric(df['carbon_monoxide'], errors='coerce')\n",
    "        df['nitrogen_dioxide'] = pd.to_numeric(df['nitrogen_dioxide'], errors='coerce')\n",
    "        df['extraction_timestamp'] = datetime.now()\n",
    "        df['latitude'] = LATITUDE\n",
    "        df['longitude'] = LONGITUDE\n",
    "        df['location'] = \"São Paulo\"\n",
    "        \n",
    "        # Remover valores nulos\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=['pm10', 'timestamp'])\n",
    "        final_count = len(df)\n",
    "        \n",
    "        print(f\"Dados de qualidade do ar transformados:\")\n",
    "        print(f\"Registros iniciais: {initial_count}\")\n",
    "        print(f\"Registros após limpeza: {final_count}\")\n",
    "        print(f\"Colunas: {list(df.columns)}\")\n",
    "        print(f\"Período: {df['timestamp'].min()} até {df['timestamp'].max()}\")\n",
    "        \n",
    "        # Visualizar amostra dos dados transformados\n",
    "        print(\"Amostra dos dados de qualidade do ar transformados:\")\n",
    "        display(df.head(5))\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na transformação de dados de qualidade do ar: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21c61e7-2266-4321-ba79-7f816e624d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_quality_report(df, source_name):\n",
    "    \"\"\"Gera relatório de qualidade de dados\"\"\"\n",
    "    try:\n",
    "        print(f\"Gerando relatório de qualidade para {source_name}...\")\n",
    "        # Calcular métricas de qualidade\n",
    "        total_records = len(df)\n",
    "        null_counts = {}\n",
    "        \n",
    "        for column in df.columns:\n",
    "            null_counts[column] = int(df[column].isnull().sum())  # Converter para int nativo\n",
    "        \n",
    "        # Estatísticas básicas para colunas numéricas\n",
    "        numeric_stats = {}\n",
    "        numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "        for col in numeric_columns:\n",
    "            numeric_stats[col] = {\n",
    "                'min': float(df[col].min()),\n",
    "                'max': float(df[col].max()),\n",
    "                'mean': float(df[col].mean()),\n",
    "                'std': float(df[col].std())\n",
    "            }\n",
    "        \n",
    "        # Converter tipos de dados para serialização JSON\n",
    "        data_types = {}\n",
    "        for col, dtype in df.dtypes.items():\n",
    "            data_types[col] = str(dtype)\n",
    "        \n",
    "        # Criar relatório\n",
    "        report = {\n",
    "            \"source\": source_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_records\": int(total_records),\n",
    "            \"null_counts\": null_counts,\n",
    "            \"numeric_statistics\": numeric_stats,\n",
    "            \"columns\": list(df.columns),\n",
    "            \"data_types\": data_types\n",
    "        }\n",
    "        \n",
    "        # Salvar relatório\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_filename = f\"{bronze_path}/quality_report_{source_name}_{timestamp}.json\"\n",
    "        \n",
    "        # Usar dbutils para salvar no Volume\n",
    "        dbutils.fs.put(report_filename, json.dumps(report, indent=2), overwrite=True)\n",
    "            \n",
    "        print(f\"Relatório de qualidade salvo em: {report_filename}\")\n",
    "        \n",
    "        # Visualizar resumo do relatório\n",
    "        print(\"Resumo do Relatório de Qualidade:\")\n",
    "        print(f\"Total de registros: {total_records}\")\n",
    "        print(f\"Colunas com valores nulos:\")\n",
    "        for col, null_count in null_counts.items():\n",
    "            if null_count > 0:\n",
    "                print(f\"     - {col}: {null_count} nulos\")\n",
    "        \n",
    "        return report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na geração do relatório de qualidade: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889e7a63-6d2d-41ff-ac0a-58667d4db833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Executa o pipeline completo\"\"\"\n",
    "    print(\"Iniciando pipeline de dados...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extração\n",
    "    print(\"\\nETAPA 1: EXTRAÇÃO\")\n",
    "    print(\"-\" * 30)\n",
    "    weather_data = extract_weather_data()\n",
    "    air_quality_data = extract_air_quality_data()\n",
    "    \n",
    "    # Transformação\n",
    "    print(\"\\nETAPA 2: TRANSFORMAÇÃO\")\n",
    "    print(\"-\" * 30)\n",
    "    weather_df = transform_weather_data(weather_data)\n",
    "    air_quality_df = transform_air_quality_data(air_quality_data)\n",
    "    \n",
    "    # Qualidade de dados\n",
    "    print(\"\\n ETAPA 3: QUALIDADE DE DADOS\")\n",
    "    print(\"-\" * 30)\n",
    "    if weather_df is not None:\n",
    "        weather_report = generate_data_quality_report(weather_df, \"weather_data\")\n",
    "    \n",
    "    if air_quality_df is not None:\n",
    "        air_quality_report = generate_data_quality_report(air_quality_df, \"air_quality_data\")\n",
    "    \n",
    "    # Salvamento Silver\n",
    "    print(\"\\n ETAPA 4: CAMADA SILVER\")\n",
    "    print(\"-\" * 30)\n",
    "    if weather_df is not None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        silver_weather_file = f\"{silver_path}/weather_data_{timestamp}.parquet\"\n",
    "        \n",
    "        # Salvar usando Spark\n",
    "        spark_df = spark.createDataFrame(weather_df)\n",
    "        spark_df.write.mode(\"overwrite\").format(\"parquet\").save(silver_weather_file)\n",
    "        print(f\" Dados meteorológicos salvos na silver: {silver_weather_file}\")\n",
    "        \n",
    "        # Visualizar dados silver\n",
    "        print(\" Dados salvos na Silver (Weather):\")\n",
    "        display(spark_df.limit(5))\n",
    "    \n",
    "    if air_quality_df is not None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        silver_air_quality_file = f\"{silver_path}/air_quality_data_{timestamp}.parquet\"\n",
    "        \n",
    "        # Salvar usando Spark\n",
    "        spark_df = spark.createDataFrame(air_quality_df)\n",
    "        spark_df.write.mode(\"overwrite\").format(\"parquet\").save(silver_air_quality_file)\n",
    "        print(f\" Dados de qualidade do ar salvos na silver: {silver_air_quality_file}\")\n",
    "        \n",
    "        # Visualizar dados silver\n",
    "        print(\" Dados salvos na Silver (Air Quality):\")\n",
    "        display(spark_df.limit(5))\n",
    "    \n",
    "    # Criar dados gold\n",
    "    print(\"\\n ETAPA 5: CAMADA GOLD\")\n",
    "    print(\"-\" * 30)\n",
    "    if weather_df is not None and air_quality_df is not None:\n",
    "        create_gold_data(weather_df, air_quality_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PIPELINE CONCLUÍDO COM SUCESSO!\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "def create_gold_data(weather_df, air_quality_df):\n",
    "    \"\"\"Cria dados finais na camada gold\"\"\"\n",
    "    try:\n",
    "        print(\" Criando camada gold...\")\n",
    "        \n",
    "        # Dados meteorológicos gold\n",
    "        weather_gold = weather_df[['timestamp', 'date', 'temperature_2m', \n",
    "                                 'relative_humidity_2m', 'pressure_msl', \n",
    "                                 'precipitation', 'location', 'latitude', 'longitude']].copy()\n",
    "        \n",
    "        # Dados qualidade do ar gold\n",
    "        air_quality_gold = air_quality_df[['timestamp', 'date', 'pm10', 'pm2_5', \n",
    "                                         'carbon_monoxide', 'nitrogen_dioxide',\n",
    "                                         'location', 'latitude', 'longitude']].copy()\n",
    "        \n",
    "        # Salvar dados gold\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        gold_weather_file = f\"{gold_path}/weather_gold_{timestamp}.parquet\"\n",
    "        gold_air_quality_file = f\"{gold_path}/air_quality_gold_{timestamp}.parquet\"\n",
    "        \n",
    "        # Salvar usando Spark\n",
    "        spark_weather = spark.createDataFrame(weather_gold)\n",
    "        spark_air_quality = spark.createDataFrame(air_quality_gold)\n",
    "        \n",
    "        spark_weather.write.mode(\"overwrite\").format(\"parquet\").save(gold_weather_file)\n",
    "        spark_air_quality.write.mode(\"overwrite\").format(\"parquet\").save(gold_air_quality_file)\n",
    "        \n",
    "        print(\"Dados gold criados com sucesso!\")\n",
    "        print(f\"  Weather Gold: {gold_weather_file}\")\n",
    "        print(f\"  Air Quality Gold: {gold_air_quality_file}\")\n",
    "        \n",
    "        # Visualização completa dos dados gold\n",
    "        print(\"\\n DADOS GOLD - VISUALIZAÇÃO COMPLETA\")\n",
    "        print(\" DADOS METEOROLÓGICOS GOLD:\")\n",
    "        display(spark_weather.limit(10))\n",
    "        \n",
    "        print(\" DADOS DE QUALIDADE DO AR GOLD:\")\n",
    "        display(spark_air_quality.limit(10))\n",
    "        \n",
    "        # Estatísticas resumidas\n",
    "        print(\" ESTATÍSTICAS RESUMIDAS:\")\n",
    "        print(\"Temperatura média:\", spark_weather.select(avg('temperature_2m')).collect()[0][0])\n",
    "        print(\"PM2.5 médio:\", spark_air_quality.select(avg('pm2_5')).collect()[0][0])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro na criação dos dados gold: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "286e203f-af06-426c-b1c6-51414c5e517d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Executar pipeline\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90c2e29-8c30-4416-a543-acf34a8d3805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar arquivos criados\n",
    "def list_files(directory):\n",
    "    \"\"\"Lista arquivos no diretório\"\"\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(directory)\n",
    "        print(f\" Arquivos em {directory}:\")\n",
    "        for file in files:\n",
    "            print(f\"  - {file.name} ({file.size} bytes)\")\n",
    "        return len(files)\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao listar {directory}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "print(\" VERIFICAÇÃO FINAL DOS ARQUIVOS CRIADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n CAMADA BRONZE:\")\n",
    "bronze_count = list_files(bronze_path)\n",
    "\n",
    "print(\"\\n CAMADA SILVER:\")\n",
    "silver_count = list_files(silver_path)\n",
    "\n",
    "print(\"\\n CAMADA GOLD:\")\n",
    "gold_count = list_files(gold_path)\n",
    "\n",
    "print(f\"\\n RESUMO FINAL:\")\n",
    "print(f\"Bronze: {bronze_count} arquivos\")\n",
    "print(f\" Silver: {silver_count} arquivos\")\n",
    "print(f\" Gold: {gold_count} arquivos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a87106d-d3e0-4404-bd40-1b5be1adda0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Ler dados gold para visualização\n",
    "    gold_weather_files = [f.path for f in dbutils.fs.ls(gold_path) if 'weather_gold' in f.name]\n",
    "    gold_air_files = [f.path for f in dbutils.fs.ls(gold_path) if 'air_quality_gold' in f.name]\n",
    "    \n",
    "    if gold_weather_files and gold_air_files:\n",
    "        weather_df = spark.read.parquet(gold_weather_files[0])\n",
    "        air_df = spark.read.parquet(gold_air_files[0])\n",
    "        \n",
    "        print(\" GRÁFICOS DOS DADOS GOLD\")\n",
    "        print(\"  Variação da Temperatura:\")\n",
    "        display(weather_df.select('timestamp', 'temperature_2m').orderBy('timestamp').limit(24))\n",
    "        \n",
    "        print(\"  Níveis de PM2.5:\")\n",
    "        display(air_df.select('timestamp', 'pm2_5').orderBy('timestamp').limit(24))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"  Visualização adicional não disponível: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) NB-FAST-TRACK-ENGENHARIA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
