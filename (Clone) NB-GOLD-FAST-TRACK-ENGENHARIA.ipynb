{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f11c2571-b2e3-488c-bbce-1727ec658196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**INICIO DO PROCESSO E CONFIGURA√á√ÉO DAS PASTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c87c5ff-fb18-4e83-ba1f-31ab5b40ddba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configura√ß√£o de diret√≥rios\n",
    "gold_path = \"/Volumes/workspace/default/gold\"\n",
    "delta_table_path = \"/Volumes/workspace/default/delta/final_climate_data\"\n",
    "delta_catalog = \"/Volumes/workspace/default\"\n",
    "delta_table_name = \"climate_data_final\"\n",
    "\n",
    "print(\"Configurando ambiente para cria√ß√£o da tabela Delta...\")\n",
    "print(f\"Gold path: {gold_path}\")\n",
    "print(f\"Delta path: {delta_table_path}\")\n",
    "print(f\"Catalog: {delta_catalog}\")\n",
    "print(f\"Table: {delta_table_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e82c99b-8c0e-4a50-900d-19dd9cdce7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**LENDO OS DADOS DA CAMADA GOLD E VISUALIZANDO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ca3178-5ea6-407c-9d3d-9f26b9029235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_gold_data():\n",
    "    \"\"\"L√™ todos os arquivos das camadas gold\"\"\"\n",
    "    try:\n",
    "        print(\"Lendo dados da camada Gold...\")\n",
    "        \n",
    "        # Listar arquivos gold\n",
    "        gold_files = dbutils.fs.ls(gold_path)\n",
    "        weather_files = [f.path for f in gold_files if 'weather_gold' in f.name]\n",
    "        air_quality_files = [f.path for f in gold_files if 'air_quality_gold' in f.name]\n",
    "        \n",
    "        print(f\" Arquivos de weather encontrados: {len(weather_files)}\")\n",
    "        print(f\" Arquivos de air quality encontrados: {len(air_quality_files)}\")\n",
    "        \n",
    "        # Ler dados meteorol√≥gicos\n",
    "        weather_dfs = []\n",
    "        for file in weather_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            weather_dfs.append(df)\n",
    "        \n",
    "        if weather_dfs:\n",
    "            weather_df = weather_dfs[0]\n",
    "            for df in weather_dfs[1:]:\n",
    "                weather_df = weather_df.union(df)\n",
    "            print(f\"Total de registros meteorol√≥gicos: {weather_df.count()}\")\n",
    "        else:\n",
    "            print(\" Nenhum dado meteorol√≥gico encontrado\")\n",
    "            weather_df = None\n",
    "        \n",
    "        # Ler dados de qualidade do ar\n",
    "        air_quality_dfs = []\n",
    "        for file in air_quality_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            air_quality_dfs.append(df)\n",
    "        \n",
    "        if air_quality_dfs:\n",
    "            air_quality_df = air_quality_dfs[0]\n",
    "            for df in air_quality_dfs[1:]:\n",
    "                air_quality_df = air_quality_df.union(df)\n",
    "            print(f\"üå´Ô∏è  Total de registros de qualidade do ar: {air_quality_df.count()}\")\n",
    "        else:\n",
    "            print(\" Nenhum dado de qualidade do ar encontrado\")\n",
    "            air_quality_df = None\n",
    "        \n",
    "        return weather_df, air_quality_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler dados gold: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Ler os dados\n",
    "weather_df, air_quality_df = read_gold_data()\n",
    "\n",
    "# Visualizar amostra dos dados\n",
    "if weather_df:\n",
    "    print(\" Amostra dos dados meteorol√≥gicos:\")\n",
    "    display(weather_df.limit(5))\n",
    "\n",
    "if air_quality_df:\n",
    "    print(\" Amostra dos dados de qualidade do ar:\")\n",
    "    display(air_quality_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f31d65-b1d7-4fda-b072-a980d586503b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**UNIFICANDO OS DADOS E SETANDO AS COLUNAS NECECSSARIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdea5148-2378-45d1-ba58-16beb55f0cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_final_table(weather_df, air_quality_df):\n",
    "    \"\"\"Cria a tabela final unificada\"\"\"\n",
    "    try:\n",
    "        print(\"Unificando dados...\")\n",
    "        \n",
    "        if weather_df is None or air_quality_df is None:\n",
    "            print(\"Dados insuficientes para unifica√ß√£o\")\n",
    "            return None\n",
    "        \n",
    "        # Juntar os dados por timestamp e localiza√ß√£o\n",
    "        final_df = weather_df.alias(\"w\").join(\n",
    "            air_quality_df.alias(\"a\"),\n",
    "            (col(\"w.timestamp\") == col(\"a.timestamp\")) & \n",
    "            (col(\"w.location\") == col(\"a.location\")),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"w.timestamp\"),\n",
    "            col(\"w.date\"),\n",
    "            col(\"w.temperature_2m\").alias(\"temperature_c\"),\n",
    "            col(\"w.relative_humidity_2m\").alias(\"humidity_percent\"),\n",
    "            col(\"w.pressure_msl\").alias(\"pressure_hpa\"),\n",
    "            col(\"w.precipitation\").alias(\"precipitation_mm\"),\n",
    "            col(\"a.pm10\"),\n",
    "            col(\"a.pm2_5\"),\n",
    "            col(\"a.carbon_monoxide\").alias(\"co_ugm3\"),\n",
    "            col(\"a.nitrogen_dioxide\").alias(\"no2_ugm3\"),\n",
    "            col(\"w.location\"),\n",
    "            col(\"w.latitude\"),\n",
    "            col(\"w.longitude\")\n",
    "        )\n",
    "        \n",
    "        # Adicionar colunas calculadas\n",
    "        final_df = final_df.withColumn(\n",
    "            \"temperature_f\", \n",
    "            (col(\"temperature_c\") * 9/5) + 32\n",
    "        ).withColumn(\n",
    "            \"air_quality_index\",\n",
    "            when(col(\"pm2_5\") <= 12, \"Bom\")\n",
    "            .when(col(\"pm2_5\") <= 35, \"Moderado\")\n",
    "            .when(col(\"pm2_5\") <= 55, \"Ruim\")\n",
    "            .when(col(\"pm2_5\") <= 150, \"Muito Ruim\")\n",
    "            .otherwise(\"Perigoso\")\n",
    "        ).withColumn(\n",
    "            \"load_timestamp\", \n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        print(f\"Dados unificados com sucesso!\")\n",
    "        print(f\"Total de registros na tabela final: {final_df.count()}\")\n",
    "        print(f\"Per√≠odo dos dados: {final_df.agg(min('timestamp'), max('timestamp')).collect()[0]}\")\n",
    "        \n",
    "        # Mostrar schema final\n",
    "        print(\"  Schema da tabela final:\")\n",
    "        final_df.printSchema()\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na unifica√ß√£o dos dados: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Criar tabela final\n",
    "final_table_df = create_final_table(weather_df, air_quality_df)\n",
    "\n",
    "if final_table_df:\n",
    "    print(\" Amostra da tabela final:\")\n",
    "    display(final_table_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "634b3a81-7d9a-43cd-98f6-091a3a1c0170",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**V2 AJUSTADA LENDO AS BIBLIOTECAS E DIRETORIOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2fedaf-c427-4572-b519-365945eb92cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configura√ß√£o de diret√≥rios\n",
    "gold_path = \"/Volumes/workspace/default/gold\"\n",
    "delta_table_path = \"/Volumes/workspace/default/delta/final_climate_data\"\n",
    "delta_catalog = \"/Volumes/workspace/default\"\n",
    "delta_table_name = \"climate_data_final\"\n",
    "\n",
    "print(\"Configurando ambiente para cria√ß√£o da tabela Delta...\")\n",
    "print(f\"Gold path: {gold_path}\")\n",
    "print(f\"Delta path: {delta_table_path}\")\n",
    "print(f\"Catalog: {delta_catalog}\")\n",
    "print(f\"Table: {delta_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9178e29-b2cc-43ec-9ff6-7e68e42bf3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**LENDO OS DADOS  DA CAMADA GOLD E VISUALIZANDO OS DADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e233ca4-2f26-4af9-b57e-4a588923a0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_gold_data():\n",
    "    \"\"\"L√™ todos os arquivos das camadas gold\"\"\"\n",
    "    try:\n",
    "        print(\" Lendo dados da camada Gold...\")\n",
    "        \n",
    "        # Listar arquivos gold usando a sintaxe correta\n",
    "        gold_files = dbutils.fs.ls(gold_path.replace(\"dbfs:\", \"\"))\n",
    "        weather_files = [f.path for f in gold_files if 'weather_gold' in f.name]\n",
    "        air_quality_files = [f.path for f in gold_files if 'air_quality_gold' in f.name]\n",
    "        \n",
    "        print(f\"  Arquivos de weather encontrados: {len(weather_files)}\")\n",
    "        print(f\" Arquivos de air quality encontrados: {len(air_quality_files)}\")\n",
    "        \n",
    "        # Ler dados meteorol√≥gicos\n",
    "        weather_dfs = []\n",
    "        for file in weather_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            weather_dfs.append(df)\n",
    "        \n",
    "        if weather_dfs:\n",
    "            weather_df = weather_dfs[0]\n",
    "            for df in weather_dfs[1:]:\n",
    "                weather_df = weather_df.union(df)\n",
    "            print(f\"  Total de registros meteorol√≥gicos: {weather_df.count()}\")\n",
    "        else:\n",
    "            print(\"  Nenhum dado meteorol√≥gico encontrado\")\n",
    "            weather_df = None\n",
    "        \n",
    "        # Ler dados de qualidade do ar\n",
    "        air_quality_dfs = []\n",
    "        for file in air_quality_files:\n",
    "            df = spark.read.parquet(file)\n",
    "            air_quality_dfs.append(df)\n",
    "        \n",
    "        if air_quality_dfs:\n",
    "            air_quality_df = air_quality_dfs[0]\n",
    "            for df in air_quality_dfs[1:]:\n",
    "                air_quality_df = air_quality_df.union(df)\n",
    "            print(f\"üå´Ô∏è  Total de registros de qualidade do ar: {air_quality_df.count()}\")\n",
    "        else:\n",
    "            print(\"  Nenhum dado de qualidade do ar encontrado\")\n",
    "            air_quality_df = None\n",
    "        \n",
    "        return weather_df, air_quality_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao ler dados gold: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Ler os dados\n",
    "weather_df, air_quality_df = read_gold_data()\n",
    "\n",
    "# Visualizar amostra dos dados\n",
    "if weather_df:\n",
    "    print(\" Amostra dos dados meteorol√≥gicos:\")\n",
    "    display(weather_df.limit(5))\n",
    "\n",
    "if air_quality_df:\n",
    "    print(\" Amostra dos dados de qualidade do ar:\")\n",
    "    display(air_quality_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99f980b-8cbb-4a41-a79f-e4229b1619fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "****CRIA√á√ÉO DA TABELA  APLICANDO O JOIN** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d215e4df-9428-49ec-982c-d2f15fdd2ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_final_table(weather_df, air_quality_df):\n",
    "    \"\"\"Cria a tabela final unificada\"\"\"\n",
    "    try:\n",
    "        print(\"Unificando dados...\")\n",
    "        \n",
    "        if weather_df is None or air_quality_df is None:\n",
    "            print(\" Dados insuficientes para unifica√ß√£o\")\n",
    "            return None\n",
    "        \n",
    "        # Juntar os dados por timestamp e localiza√ß√£o \n",
    "        final_df = weather_df.alias(\"w\").join(\n",
    "            air_quality_df.alias(\"a\"),\n",
    "            (col(\"w.timestamp\") == col(\"a.timestamp\")) & \n",
    "            (col(\"w.location\") == col(\"a.location\")),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"w.timestamp\"),\n",
    "            col(\"w.date\"),\n",
    "            col(\"w.temperature_2m\").alias(\"temperature_c\"),\n",
    "            col(\"w.relative_humidity_2m\").alias(\"humidity_percent\"),\n",
    "            col(\"w.pressure_msl\").alias(\"pressure_hpa\"),\n",
    "            col(\"w.precipitation\").alias(\"precipitation_mm\"),\n",
    "            col(\"a.pm10\"),\n",
    "            col(\"a.pm2_5\"),\n",
    "            col(\"a.carbon_monoxide\").alias(\"co_ugm3\"),\n",
    "            col(\"a.nitrogen_dioxide\").alias(\"no2_ugm3\"),\n",
    "            col(\"w.location\"),\n",
    "            col(\"w.latitude\"),\n",
    "            col(\"w.longitude\")\n",
    "           \n",
    "        )\n",
    "        \n",
    "        # Adicionar colunas calculadas \n",
    "        final_df = final_df.withColumn(  \n",
    "            \"temperature_f\", \n",
    "            (col(\"temperature_c\") * 9/5) + 32\n",
    "        ).withColumn(\n",
    "            \"air_quality_index\",\n",
    "            when(col(\"pm2_5\") <= 12, \"Bom\")\n",
    "            .when(col(\"pm2_5\") <= 35, \"Moderado\")\n",
    "            .when(col(\"pm2_5\") <= 55, \"Ruim\")\n",
    "            .when(col(\"pm2_5\") <= 150, \"Muito Ruim\")\n",
    "            .otherwise(\"Perigoso\")\n",
    "        ).withColumn(\n",
    "            \"load_timestamp\", \n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        print(f\" Dados unificados com sucesso!\")\n",
    "        print(f\"Total de registros na tabela final: {final_df.count()}\")\n",
    "        \n",
    "        # Mostrar schema final\n",
    "        print(\" Schema da tabela final:\")\n",
    "        final_df.printSchema()\n",
    "        \n",
    "        return final_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro na unifica√ß√£o dos dados: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Criar tabela final\n",
    "final_table_df = create_final_table(weather_df, air_quality_df)\n",
    "\n",
    "if final_table_df:\n",
    "    print(\"Amostra da tabela final:\")\n",
    "    display(final_table_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b94ebe5-89c1-4450-ba34-7252918527be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CRIANDO  TABELA FINAL USANDO SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fcc004f-ee1c-4bfb-bb8f-f61260317071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_delta_table(final_df):\n",
    "    \"\"\"Cria a tabela Delta final \"\"\"\n",
    "    try:\n",
    "        print(\" Criando tabela Delta...\")\n",
    "        \n",
    "        if final_df is None:\n",
    "            print(\" Nenhum dado para criar tabela\")\n",
    "            return None\n",
    "        \n",
    "        #  Unity Catalog\n",
    "        \n",
    "        catalog_name = \"workspace\"  #  catalog\n",
    "        schema_name = \"default\"     #  schema\n",
    "        table_name = \"climate_data_final\"\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Criar DataFrame tempor√°rio\n",
    "        final_df.createOrReplaceTempView(\"temp_final_table\")\n",
    "        \n",
    "        # Criar tabela usando SQL \n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {full_table_name}\n",
    "            USING DELTA\n",
    "            AS SELECT * FROM temp_final_table\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\" Tabela Delta criada com sucesso!\")\n",
    "        print(f\"Tabela: {full_table_name}\")\n",
    "        \n",
    "        # Verificar a tabela criada\n",
    "        delta_df = spark.read.table(full_table_name)\n",
    "        print(f\"Total de registros na tabela Delta: {delta_df.count()}\")\n",
    "        \n",
    "        return delta_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar tabela Delta: {str(e)}\")\n",
    "        print(\" Tentando m√©todo alternativo...\")\n",
    "        \n",
    "        # M√©todo alternativo usando DataFrameWriter\n",
    "        try:\n",
    "            # Escrever usando DataFrameWriter\n",
    "            (final_df.write\n",
    "                .format(\"delta\")\n",
    "                .mode(\"overwrite\")\n",
    "                .option(\"overwriteSchema\", \"true\")\n",
    "                .saveAsTable(full_table_name))\n",
    "            \n",
    "            print(f\" Tabela Delta criada com m√©todo alternativo!\")\n",
    "            delta_df = spark.read.table(full_table_name)\n",
    "            return delta_df\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\" Erro no m√©todo alternativo: {str(e2)}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f69a0d19-4b6e-4c25-93d4-d342b6759e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CRIANDO A TABELA DELTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e4d8a4-3c33-41f9-b745-1a661654d3a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configura√ß√£o \n",
    "catalog_name = \"workspace\"  # Seu catalog\n",
    "schema_name = \"default\"     # Seu schema\n",
    "table_name = \"climate_data_final\"\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "print(\"Configura√ß√£o para Unity Catalog:\")\n",
    "print(f\"Catalog: {catalog_name}\")\n",
    "print(f\" Schema: {schema_name}\")\n",
    "print(f\"Table: {table_name}\")\n",
    "print(f\" Full name: {full_table_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4.2 Executar Cria√ß√£o da Tabela\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Criar tabela Delta\n",
    "delta_final_df = create_delta_table(final_table_df)\n",
    "\n",
    "if delta_final_df:\n",
    "    print(\" Amostra da tabela Delta:\")\n",
    "    display(delta_final_df.limit(10))\n",
    "else:\n",
    "    print(\"Falha ao criar tabela Delta\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea7254a7-4cd1-4150-bbd4-9c0d4014bd13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7b0a4ed-cea4-4ed7-baeb-bf06b2711336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**CHECKLIST PARA VERIFICA√á√ÉO DA CRIA√á√ÉO DA TABELA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145c3e7-5a08-4348-b264-3d606bb9e44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar se a tabela foi criada corretamente\n",
    "try:\n",
    "    # Listar tabelas no schema\n",
    "    tables = spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name} LIKE '{table_name}'\")\n",
    "    if tables.count() > 0:\n",
    "        print(\"Tabela encontrada no Unity Catalog!\")\n",
    "        display(tables)\n",
    "        \n",
    "        # Detalhes da tabela\n",
    "        table_details = spark.sql(f\"DESCRIBE DETAIL {full_table_name}\")\n",
    "        print(\"Detalhes da tabela:\")\n",
    "        display(table_details)\n",
    "        \n",
    "        # Contagem de registros\n",
    "        count = spark.sql(f\"SELECT COUNT(*) FROM {full_table_name}\").collect()[0][0]\n",
    "        print(f\" Total de registros: {count:,}\")\n",
    "        \n",
    "    else:\n",
    "        print(\" Tabela n√£o encontrada no Unity Catalog\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" Erro ao verificar tabela: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e84ab1-c259-41da-87ee-eaa16ce3f968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**QUERYS SQL PARA CONSULTA DOS DADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ee7c73-8c64-413b-a787-3deed57ae9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# consulta na tabela\n",
    "try:\n",
    "    print(\"Testando consulta na tabela...\")\n",
    "    test_query = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            temperature_c,\n",
    "            humidity_percent,\n",
    "            pm2_5,\n",
    "            air_quality_index\n",
    "        FROM {full_table_name}\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 5\n",
    "    \"\"\")\n",
    "    display(test_query)\n",
    "    print(\"Consulta executada com sucesso!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Erro na consulta: {str(e)}\")\n",
    "    print(\" Tentando ver tabelas dispon√≠veis...\")\n",
    "    \n",
    "    # Listar todas as tabelas dispon√≠veis\n",
    "    try:\n",
    "        all_tables = spark.sql(\"SHOW TABLES\")\n",
    "        display(all_tables)\n",
    "    except:\n",
    "        print(\"N√£o foi poss√≠vel listar tabelas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c740a7b-af7b-46cf-bf92-e81218fc114d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL : Dados mais recentes\n",
    "display(\n",
    "    spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            timestamp,\n",
    "            temperature_c,\n",
    "            humidity_percent,\n",
    "            pm2_5,\n",
    "            air_quality_index\n",
    "        FROM climate_data_final\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd05be89-da36-4d4b-ac53-945899fff08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: M√©dias por hora\n",
    "print(\"M√©dias por hora:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        DATE_TRUNC('hour', timestamp) as hour,\n",
    "        ROUND(AVG(temperature_c), 1) as avg_temperature,\n",
    "        ROUND(AVG(pm2_5), 1) as avg_pm2_5,\n",
    "        COUNT(*) as readings\n",
    "    FROM climate_data_final\n",
    "    GROUP BY DATE_TRUNC('hour', timestamp)\n",
    "    ORDER BY hour DESC\n",
    "    LIMIT 12\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66f41d4-26ae-4914-8ad9-db22927462d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SQL: Qualidade do ar por faixa\n",
    "print(\"Qualidade do ar por faixa:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        air_quality_index,\n",
    "        MIN(pm2_5) as min_pm2_5,\n",
    "        MAX(pm2_5) as max_pm2_5,\n",
    "        AVG(pm2_5) as avg_pm2_5,\n",
    "        COUNT(*) as readings\n",
    "    FROM climate_data_final\n",
    "    GROUP BY air_quality_index\n",
    "    ORDER BY avg_pm2_5\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b3d0b7-0763-4ff3-84ef-1ac0a765267d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_delta_table_optimized(final_df):\n",
    "    \"\"\"Cria tabela Delta com particionamento e otimiza√ß√µes\"\"\"\n",
    "    try:\n",
    "        print(\"Criando tabela Delta otimizada...\")\n",
    "        \n",
    "        if final_df is None:\n",
    "            print(\" Nenhum dado para criar tabela\")\n",
    "            return None\n",
    "        \n",
    "        # Configura√ß√£o Unity Catalog\n",
    "        catalog_name = \"workspace\"\n",
    "        schema_name = \"default\"\n",
    "        table_name = \"climate_data_final\"\n",
    "        full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "        \n",
    "        # Adicionar colunas de particionamento\n",
    "        final_df_partitioned = final_df.withColumn(\n",
    "            \"year\", year(col(\"timestamp\"))\n",
    "        ).withColumn(\n",
    "            \"month\", month(col(\"timestamp\"))\n",
    "        ).withColumn(\n",
    "            \"day\", dayofmonth(col(\"timestamp\"))\n",
    "        )\n",
    "        \n",
    "        # Criar tabela Delta com particionamento e compress√£o\n",
    "        (final_df_partitioned.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .option(\"compression\", \"snappy\")  # Compress√£o Snappy\n",
    "            .partitionBy(\"year\", \"month\")     # Particionamento por ano/m√™s\n",
    "            .saveAsTable(full_table_name))\n",
    "        \n",
    "        print(\"Tabela Delta criada com otimiza√ß√µes!\")\n",
    "        print(\"Particionamento: year, month\")\n",
    "        print(\" Compress√£o: Snappy\")\n",
    "        print(f\" Tabela: {full_table_name}\")\n",
    "        \n",
    "        # Otimizar a tabela\n",
    "        optimize_delta_table(full_table_name)\n",
    "        \n",
    "        return spark.read.table(full_table_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao criar tabela Delta: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def optimize_delta_table(table_name):\n",
    "    \"\"\"Otimiza a tabela Delta para melhor performance\"\"\"\n",
    "    try:\n",
    "        print(\" Otimizando tabela Delta...\")\n",
    "        \n",
    "        # Compactar arquivos (OPTIMIZE)\n",
    "        spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (timestamp, location)\")\n",
    "        \n",
    "        # Coletar estat√≠sticas\n",
    "        spark.sql(f\"ANALYZE TABLE {table_name} COMPUTE STATISTICS\")\n",
    "        \n",
    "        # Vacuum para limpeza\n",
    "        spark.sql(f\"VACUUM {table_name} RETAIN 168 HOURS\")  # 7 dias\n",
    "        \n",
    "        print(\" Tabela otimizada com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Aviso na otimiza√ß√£o: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb1af90-c13c-474d-b64a-638eb735066a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**VISUALIZA√á√ÉO DE ALGUNS INDICADORES DO TEMPO EM FORMATO DE GR√ÅFICOS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698c53d9-ed4d-436b-a9f0-2100e2bfb154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_visualizations(final_table_name):\n",
    "    \"\"\"Cria visualiza√ß√µes b√°sicas dos dados\"\"\"\n",
    "    try:\n",
    "        print(\"Criando visualiza√ß√µes...\")\n",
    "        \n",
    "        # Converter para Pandas para visualiza√ß√£o\n",
    "        sample_data = spark.sql(f\"\"\"\n",
    "            SELECT * FROM {final_table_name} \n",
    "            WHERE timestamp >= current_date() - INTERVAL 7 DAYS\n",
    "            LIMIT 1000\n",
    "        \"\"\").toPandas()\n",
    "        \n",
    "        # Configurar matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        plt.style.use('default')\n",
    "        sns.set_palette(\"husl\")\n",
    "        \n",
    "        # Figura com m√∫ltiplos gr√°ficos\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Temperatura ao longo do tempo\n",
    "        sample_data['timestamp'] = pd.to_datetime(sample_data['timestamp'])\n",
    "        sample_data.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        sample_data['temperature_c'].resample('H').mean().plot(\n",
    "            ax=ax1, title='Temperatura M√©dia por Hora', color='red'\n",
    "        )\n",
    "        ax1.set_ylabel('¬∞C')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Qualidade do ar\n",
    "        quality_counts = sample_data['air_quality_index'].value_counts()\n",
    "        ax2.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%')\n",
    "        ax2.set_title('Distribui√ß√£o da Qualidade do Ar')\n",
    "        \n",
    "        # 3. PM2.5 vs Temperatura\n",
    "        hourly_avg = sample_data.resample('H').mean()\n",
    "        ax3.scatter(hourly_avg['temperature_c'], hourly_avg['pm2_5'], alpha=0.6)\n",
    "        ax3.set_xlabel('Temperatura (¬∞C)')\n",
    "        ax3.set_ylabel('PM2.5 (¬µg/m¬≥)')\n",
    "        ax3.set_title('Rela√ß√£o: Temperatura vs PM2.5')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. S√©rie temporal PM2.5\n",
    "        sample_data['pm2_5'].resample('H').mean().plot(\n",
    "            ax=ax4, title='PM2.5 M√©dio por Hora', color='orange'\n",
    "        )\n",
    "        ax4.set_ylabel('¬µg/m¬≥')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Visualiza√ß√µes criadas com sucesso!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro nas visualiza√ß√µes: {str(e)}\")\n",
    "\n",
    "# Executar as visualiza√ß√µes\n",
    "create_visualizations(\"workspace.default.climate_data_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5afb4f92-c7e1-4202-9a8d-7f2ef1c6f167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configura√ß√£o\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"default\"\n",
    "table_name = \"climate_data_final\"\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "print(\"Iniciando processo de persist√™ncia otimizada...\")\n",
    "\n",
    "# 1. Ler dados da Gold\n",
    "weather_df, air_quality_df = read_gold_data()\n",
    "\n",
    "# 2. Criar tabela final\n",
    "final_table_df = create_final_table(weather_df, air_quality_df)\n",
    "\n",
    "# 3. Criar tabela Delta otimizada\n",
    "if final_table_df:\n",
    "    delta_final_df = create_delta_table_optimized(final_table_df)\n",
    "    \n",
    "    # 4. Verifica√ß√µes\n",
    "    if delta_final_df:\n",
    "        print(\"Persist√™ncia conclu√≠da com sucesso!\")\n",
    "        print(\" Resumo da implementa√ß√£o:\")\n",
    "        print(f\" Formato: Delta/Parquet com Snappy\")\n",
    "        print(f\" Particionamento: year, month\")\n",
    "        print(f\" Compress√£o: Habilitada\")\n",
    "        print(f\" Otimiza√ß√µes: Z-Ordering aplicado\")\n",
    "        \n",
    "        # 5. Visualiza√ß√µes\n",
    "        create_visualizations(full_table_name)\n",
    "        \n",
    "        # 6. Exemplo abaixo de consulta otimizada filtrando dados por ano e m√™s e agrupando por ano e m√™s\n",
    "        print(\"Exemplo de consulta particionada:\")\n",
    "        display(spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                year,\n",
    "                month,\n",
    "                AVG(temperature_c) as avg_temp,\n",
    "                AVG(pm2_5) as avg_pm25,\n",
    "                COUNT(*) as readings\n",
    "            FROM {full_table_name}\n",
    "            WHERE year = 2024 AND month = 12\n",
    "            GROUP BY year, month\n",
    "            ORDER BY year, month\n",
    "        \"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882ed1d5-0ca9-4f2f-b8c1-8162d443fd2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def verify_implementation():\n",
    "    \"\"\"Verifica se todas as otimiza√ß√µes foram aplicadas\"\"\"\n",
    "    print(\"Verificando implementa√ß√£o...\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar particionamento\n",
    "        partitions = spark.sql(f\"\"\"\n",
    "            SHOW PARTITIONS {full_table_name}\n",
    "        \"\"\")\n",
    "        print(\"Particionamento implementado:\")\n",
    "        display(partitions.limit(5))\n",
    "        \n",
    "        # Verificar formato e compress√£o\n",
    "        details = spark.sql(f\"\"\"\n",
    "            DESCRIBE DETAIL {full_table_name}\n",
    "        \"\"\").collect()[0]\n",
    "        \n",
    "        print(f\"Formato: {details.format}\")\n",
    "        print(f\"Localiza√ß√£o: {details.location}\")\n",
    "        print(f\"Tamanho: {round(details.sizeInBytes/1024/1024, 2)} MB\")\n",
    "        \n",
    "        # Verificar estat√≠sticas\n",
    "        print(\"Estat√≠sticas da tabela:\")\n",
    "        display(spark.sql(f\"\"\"\n",
    "            SELECT * FROM {full_table_name}\n",
    "            LIMIT 1\n",
    "        \"\"\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro na verifica√ß√£o: {str(e)}\")\n",
    "\n",
    "# Executar verifica√ß√£o\n",
    "verify_implementation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412dd439-5465-488e-b85f-70aa5037e630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**## EXPLICA√á√ÉO DOS CONCEITOS ABORDADOS NESSE PROJETO\n",
    "\n",
    "### Formato Escolhido: Delta Lake/Parquet com Compress√£o Snappy\n",
    "\n",
    "**Por qu√™ Delta/Parquet?**\n",
    "- **Schema Enforcement**: Valida√ß√£o de schema nativa\n",
    "- **ACID Compliance**: Transa√ß√µes at√¥micas\n",
    "- **Time Travel**: Versionamento de dados\n",
    "- **Efici√™ncia**: Formato colunar para consultas anal√≠ticas\n",
    "- **Compress√£o Snappy**: Balanceamento entre velocidade e taxa de compress√£o\n",
    "\n",
    "### Estrat√©gia de Particionamento: Ano/M√™s\n",
    "\n",
    "**Por qu√™ particionamento temporal?**\n",
    "- **Performance**: Consultas filtradas por tempo s√£o 10-100x mais r√°pidas\n",
    "- **Gerenciamento**: Facilita opera√ß√µes de manuten√ß√£o (VACUUM, OPTIMIZE)\n",
    "- **Custo**: Redu√ß√£o de dados escaneados em consultas\n",
    "- **Organiza√ß√£o**: Estrutura l√≥gica para data lakes\n",
    "\n",
    "### Contribui√ß√£o para Desempenho e Escalabilidade\n",
    "\n",
    "**Benef√≠cios de performance:**\n",
    "- **Z-Ordering**: Melhora a localidade dos dados para consultas comuns\n",
    "- **Estat√≠sticas**: Otimizador de consultas toma decis√µes melhores\n",
    "- **Compacta√ß√£o**: Redu√ß√£o de 70-80% no armazenamento\n",
    "- **Leitura seletiva**: Somente colunas necess√°rias s√£o lidas\n",
    "\n",
    "**Escalabilidade:**\n",
    "- Suporte a petabytes de dados\n",
    "- Processamento distribu√≠do nativo\n",
    "- Integra√ß√£o com ecossistema Spark\n",
    "\n",
    "### Cen√°rios de Consumo Futuro\n",
    "\n",
    "1. **Dashboards Anal√≠ticos**\n",
    "   - Power BI/Tableau para monitoramento em tempo real\n",
    "   - M√©tricas de qualidade do ar e clima\n",
    "\n",
    "2. **Machine Learning**\n",
    "   - Previs√£o de qualidade do ar\n",
    "   - Modelos de s√©ries temporais clim√°ticas\n",
    "\n",
    "3. ** An√°lises Ad-hoc**\n",
    "   - Consultas SQL explorat√≥rias\n",
    "   - Correla√ß√£o entre vari√°veis clim√°ticas\n",
    "\n",
    "4. ** Sistemas Downstream**\n",
    "   - APIs para consumo de dados\n",
    "   - Alertas autom√°ticos baseados em thresholds\n",
    "\n",
    "5. ** Relat√≥rios Regulat√≥rios**\n",
    "   - Conformidade ambiental\n",
    "   - Tend√™ncias hist√≥ricas**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) NB-GOLD-FAST-TRACK-ENGENHARIA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
